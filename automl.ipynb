{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.train.automl.utilities import get_primary_metrics\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "\n",
    "The dataset used for this project is the [Heart Failure Prediction](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data) dataset taken from Kaggle.\n",
    "\n",
    "This dataset contains 12 features that can be used to predict mortality by heart failure:\n",
    "\n",
    "- age: Age of the patient\n",
    "- amaemia: Decrease of red blood cells or hemoglobin\n",
    "- creatinine_phosphokinase: Level of the CPK enzyme in the blood (mcg/L)\n",
    "- diabetes: If the patient has diabetes\n",
    "- ejection_fraction: Percentage of blood leaving the heart at each contraction\n",
    "- high_blood_pressure: If the patient has hypertension\n",
    "- platelets: Platelets in the blood (kiloplatelets/mL)\n",
    "- serum_creatinine: Level of serum creatinine in the blood (mg/dL)\n",
    "- serum_sodium: Level of serum sodium in the blood (mEq/L)\n",
    "- sex: Woman or man\n",
    "- smoking: If the patient smokes or not\n",
    "- time: Follow-up period (days)\n",
    "\n",
    "The target column is DEATH_EVENT which tells if the patient deceased during the follow-up period. The task performed in this project is to predict whether or not a death event occurs.\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "experiment_name = 'heart-failure-automl'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "run = exp.start_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking and printing existing compute targets\n",
    "compute_targets= ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create compute cluster\n",
    "compute_cluster_name= \"aml-compute\"\n",
    "\n",
    "#Check if compute cluster already exists\n",
    "try:\n",
    "    compute_cluster=ComputeTarget(workspace=ws, name=compute_cluster_name)\n",
    "    print(\"Found existing cluster, use it...\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new cluster...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',max_nodes=4)\n",
    "    compute_cluster = ComputeTarget.create(ws, compute_cluster_name, compute_config)\n",
    "    \n",
    "compute_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "\n",
    "# Create TabularDataset using TabularDatasetFactory\n",
    "# Data is available at: \n",
    "# \"https://raw.githubusercontent.com/neha7598/azure-ml-capstone/main/data/heart_failure_clinical_records_dataset.csv\"\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "path_to_data= \"https://raw.githubusercontent.com/neha7598/azure-ml-capstone/main/data/heart_failure_clinical_records_dataset.csv\"\n",
    "data=TabularDatasetFactory.from_delimited_files(path=path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to_pandas_dataframe()\n",
    "x=data.drop('DEATH_EVENT',axis=1)\n",
    "y=data['DEATH_EVENT']\n",
    "\n",
    "##split into train and test datasets\n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "#concatenate to form train and test datasets \n",
    "train_df=pd.concat([x_train, y_train], axis=1)\n",
    "test_df=pd.concat([x_test, y_test], axis=1)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pandas dataframe as .csv and upload to datastore\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "pd.DataFrame(train_df).to_csv(\"data/train_data.csv\", index=False)\n",
    "pd.DataFrame(test_df).to_csv(\"data/test_data.csv\", index=False)\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir='./data', target_path='heart-failure', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset as TabularDataset\n",
    "train_data = Dataset.Tabular.from_delimited_files(path=ds.path('heart-failure/train_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_primary_metrics(\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "TODO: Explain why you chose the automl settings and cofiguration you used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Put your automl settings here\n",
    "automl_settings = {\n",
    "    \"enable_early_stopping\" : True,\n",
    "    \"experiment_timeout_minutes\": 30,\n",
    "    \"featurization\": 'auto',\n",
    "    \"primary_metric\": 'accuracy',\n",
    "    \"verbosity\": logging.INFO\n",
    "}\n",
    "\n",
    "# TODO: Put your automl config here\n",
    "automl_config = AutoMLConfig(\n",
    "    task='classification',\n",
    "    training_data=train_data,\n",
    "    label_column_name='DEATH_EVENT',\n",
    "    n_cross_validations=4,\n",
    "    compute_target=compute_cluster,\n",
    "    **automl_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Submit your experiment\n",
    "remote_run = experiment.submit(automl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()\n",
    "\n",
    "best_run_metrics = best_run.get_metrics() \n",
    "for metric_name in best_run_metrics:\n",
    "    metric = best_run_metrics[metric_name]\n",
    "    print(metric_name, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model\n",
    "best_run.register_model(model_name = 'automl_best_model.pkl', model_path = './outputs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = best_run.properties['model_name']\n",
    "\n",
    "script_file_name = 'inference/score.py'\n",
    "\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', 'inference/score.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'AutoML Model trained on heart failure data to predict if death event occurs or not'\n",
    "tags = None\n",
    "model = remote_run.register_model(model_name = model_name, description = description, tags = tags)\n",
    "\n",
    "print(remote_run.model_id) # This will be written to the script file later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = InferenceConfig(entry_script=script_file_name)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {'area': \"hfData\", 'type': \"automl_classification\"}, \n",
    "                                               description = 'Heart Failure Prediction')\n",
    "\n",
    "aci_service_name = 'automl-heart-failure'\n",
    "print(aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.update(enable_app_insights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"State \"+ aci_service.state)\n",
    "print(\"Key \" + aci_service.get_keys()[0])\n",
    "print(\"Swagger URI \" + aci_service.swagger_uri)\n",
    "print(\"Scoring URI \" + aci_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# URL for the web service, should be similar to:\n",
    "# 'http://8530a665-66f3-49c8-a953-b82a2d312917.eastus.azurecontainer.io/score'\n",
    "scoring_uri = aci_service.scoring_uri\n",
    "# If the service is authenticated, set the key or token\n",
    "primary,secondary = aci_service.get_keys()\n",
    "key = primary\n",
    "\n",
    "# Two sets of data to score, so we get two results back\n",
    "data = {\"data\":\n",
    "        [\n",
    "          {\n",
    "            \"age\": 70.0,\n",
    "            \"anaemia\": 1,\n",
    "            \"creatinine_phosphokinase\": 4020,\n",
    "            \"diabetes\": 1,\n",
    "            \"ejection_fraction\": 32,\n",
    "            \"high_blood_pressure\": 1,\n",
    "            \"platelets\": 234558.23,\n",
    "            \"serum_creatinine\": 1.4,\n",
    "            \"serum_sodium\": 125,\n",
    "            \"sex\": 0,\n",
    "            \"smoking\": 1,\n",
    "            \"time\": 12\n",
    "          },\n",
    "          {\n",
    "            \"age\": 75.0,\n",
    "            \"anaemia\": 0,\n",
    "            \"creatinine_phosphokinase\": 4221,\n",
    "            \"diabetes\": 1,\n",
    "            \"ejection_fraction\": 22,\n",
    "            \"high_blood_pressure\": 0,\n",
    "            \"platelets\": 304567.23,\n",
    "            \"serum_creatinine\": 1.1,\n",
    "            \"serum_sodium\": 145,\n",
    "            \"sex\": 1,\n",
    "            \"smoking\": 0,\n",
    "            \"time\": 7\n",
    "          },\n",
    "      ]\n",
    "    }\n",
    "# Convert to JSON string\n",
    "input_data = json.dumps(data)\n",
    "with open(\"data.json\", \"w\") as _f:\n",
    "    _f.write(input_data)\n",
    "\n",
    "# Set the content type\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "# If authentication is enabled, set the authorization header\n",
    "headers['Authorization'] = f'Bearer {key}'\n",
    "\n",
    "# Make the request and display the response\n",
    "resp = requests.post(scoring_uri, input_data, headers=headers)\n",
    "print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "aci_service.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service.delete()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
